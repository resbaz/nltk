{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<img style=\"float:left\" src=\"http://ipython.org/_static/IPy_header.png\" />\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 2: Common NLTK tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "In this session we provide an quick introduction to the field of *corpus linguistics*. We then engage with common uses of NLTK within these areas, such as sentence segmentation, tokenisation and stemming. Often, NLTK has inbuilt methods for performing these tasks. As a learning exercise, however, we will sometimes build basic tools from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus linguistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though corpus linguistics has been around since the 1950s, it is only in the last 20 years that its methods have been made available to individual researchers. GUIs including [Wordsmith Tools](http://www.lexically.net/wordsmith/) and [AntConc](http://www.laurenceanthony.net/software.html). \n",
    "\n",
    "Alongside the development of GUIs, there has also been a shift from *general, balanced corpora* (corpora seeking to represent a language generally) toward *specialised corpora* (corpora containing texts of one specific type, from one speaker, etc.). More and more commonly, texts are taken from the Web.\n",
    "\n",
    "> **Note:** We'll discuss building corpora from online texts in a bit more detail tomorrow afternoon.\n",
    "\n",
    "After a long period of resistance, corpus linguistics has gained acceptence within a number of research areas. A few popular applications are within:\n",
    "\n",
    "* **Lexicography** (creating usage-based definitions of words and locating real examples)\n",
    "* **Language pedagogy** (advanced language learners can use a concordancing GUI or collocation tests to understand how certain words are used in the target language)\n",
    "* **Discourse analysis** (researching how meaning is made beyond the level of the clause/sentence)\n",
    "\n",
    "Notably, corpus linguistic methods have been embraced within the emerging paradigm of Digital Humanities, where it's sometimes called *distant reading*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpora and discourse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As hardware, software and data become more and more available, people have started using corpus linguistic methods for discourse-analytic work. Paul Baker refers the combination of corpus linguistics and (critical) discourse analysis as a [*useful methodological synergy*](#ref:baker). Corpora bring objectivity and empiricism to a qualitative, interpretative tradition, while discourse-analytic methods provide corpus linguistics with a means of contextualising abstracted results.\n",
    "\n",
    "Within this area, researchers rely on corpora to varying extents. In *corpus-driven* discourse analysis, researchers interpret the corpus based on the findings of the corpus interrogation. In *corpus-assisted* discourse analysis, researchers may use corpora to provide evidence about the way a given person/idea/discourse is commonly represented by certain people/in certain publications etc.\n",
    "\n",
    "Our work here falls under the *corpus-driven* heading, as we are exploring the dataset without any major hypotheses in mind.\n",
    "\n",
    "> **Note:** Some linguists remain skeptical of corpus linguistics generally. In a well-known critique, Henry Widdowson ([2000, p. 6-7](#ref:widdowson)) said:\n",
    ">\n",
    "> Corpus linguistics \\[...\\] (there) is no doubt that this is an immensely important development in descriptive linguistics. That is not the issue here. The quantitative analysis of text by computer reveals facts about actual language behaviour which are not, or at least not immediately, accessible to intuition. There are frequencies of occurrence of words, and regular patterns of collocational co-occurrence, which users are unaware of, though they must be part of their competence in a procedural sense since they would not otherwise be attested. They are third person observed data ('When do they use the word X?') which are different from the first person data of introspection ('When do I use the word X?'), and the second person data of elicitation ('When do you use the word X?'). Corpus analysis reveals textual facts, fascinating profiles of produced language, and its concordances are always springing surprises. They do indeed reveal a reality about language usage which was hitherto not evident to its users.\n",
    ">\n",
    "> But this achievement of corpus analysis at the same time necessarily defines its limitations. For one thing, since what is revealed is contrary to intuition, then it cannot represent the reality of first person awareness. We get third person facts of what people do, but not the facts of what people know, nor what they think they do: they come from the perspective of the observer looking on, not the introspective of the insider. In ethnomethodogical terms, we do not get member categories of description. Furthermore, it can only be one aspect of what they do that is captured by such quantitative analysis. For, obviously enough, the computer can only cope with the material products ofwhat people do when they use language. It can only analyse the textual traces of the processes whereby meaning is achieved: it cannot account for the complex interplay of linguistic and contextual factors whereby discourse is enacted. It cannot produce ethnographic descriptions of language use. In reference to Hymes's components of communicative competence (Hymes 1972), we can say that corpus analysis deals with the textually attested, but not with the encoded possible, nor the contextually appropriate.\n",
    "> \n",
    "> To point out these rather obvious limitations is not to undervalue corpus analysis but to define more clearly where its value lies. What it can do is reveal the properties of text, and that is impressive enough. But it is necessarily only a partial account of real language. For there are certain aspects of linguistic reality that it cannot reveal at all. In this respect, the linguistics of the attested is just as partial as the linguistics of the possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we have to load a corpus. We'll use a text file containing posts to an Australian online forum for discussing politics. It's full of very interesting natural language data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from IPython.display import HTML\n",
    "HTML('<iframe src=http://www.ozpolitic.com/forum/YaBB.pl?board=global width=700 height=350></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file is available online, at the [ResBaz GitHub](https://github.com/resbaz). We can ask Python to get it for us. \n",
    "\n",
    "> Later in the course, we'll discuss how to extract data from the Web and turn this data into a corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from urllib import urlopen # a library for working with urls\n",
    "url = \"https://raw.githubusercontent.com/resbaz/nltk/corpora/oz_politics/ozpol.txt\" # define the url\n",
    "raw = urlopen(url).read() # download and read the corpus into raw variable\n",
    "raw = unicode(raw.lower(), 'utf-8') # make it lowercase and unicode\n",
    "len(raw) # how many characters does it contain?\n",
    "raw[:2000] # first 2000 characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We actually already downloaded this file when we first cloned the ResBaz GitHub repository. It's in our *corpora* folder. We can access it like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u\"no greens-win, many of us right wingers want to stay the hell out of the middle east. nothing is going to stop that sh.thole of the world from tearing each others' throats out.\\nafter all, they have been doing it successfully for centuries.\\nthen you better start educating your hard right mates about greens' renewable energy. sooner we end our addiction to arab oil and middle eastern exports, like live animals, the sooner we can distance ourselves.\\ni am not saying that all muslims are like this. however, i am saying that many, many of them are. they are flying under the radar and getting strong, just like hitler's storm troopers before little adolph rose to power.\\nsensationalist right? no one thought much about little adolph and his brown shirted followers, until they stormed to power, whilst the do gooders stood by and tut tutted.\\nif you are a student of history you can see exactly what is happening here. soon it will be too late, it most probably already is.\\njust when sydney-siders thought it was safe to close their eyes and go to sleep for a night, pondering the lack of evening gun battles lately, they learn of something worse than drive by shootings.\\nas they woke to another day\\x92s work the people of sydney [and melbourne] learned of a joint operation between their states\\x92 police forces and the australian federal police, against an imminent terrorist attack in their cities.\\nfour men - all australian citizens - were arrested this morning as federal and state police, armed with search warrants, swooped on members of the suspected terror cell this morning in the second-largest counter-terrorism operation in the nation's history\\x85. about 400 police raided homes in the northern melbourne suburbs of glenroy, meadow heights, roxburgh park, broadmeadows, westmeadows, preston and epping. they also raided homes at carlton in inner melbourne and colac in southwestern victoria. (source)\\nthe intention, apparently, was to attack a sydney army base:\\nauthorities believe the group is\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open('../../corpora/oz_politics/ozpol.txt')\n",
    "raw = f.read()\n",
    "raw = unicode(raw.lower(), 'utf-8') # make it lowercase and unicode\n",
    "len(raw)\n",
    "raw[:2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, with a basic understanding of regex, we can now start to turn our corpus into a structured resource. At present, we have 'raw', a very, very long string of text.\n",
    "\n",
    " We should break the string into segments. First, we'll split the corpus into sentences. This task is a pretty boring one, and it's tough for us to improve on existing resources. We'll try, though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'post some better information.',\n",
       " u\"there's been hundreds in iraq alone.\",\n",
       " u'the iraq terrorist attacks are reported.',\n",
       " u\"you need to look harder:\\njihadists issuing their warning that the beheadings will continue if the u.s. continues to support the kurdshours after shocking world with desert execution of 300 syrian soldiers, isis parade kurds dressed in guantanamo-style suits, behead one and promise to kill them all unless usa pull out of iraq\\nsurvivor: mohammed, whose name has been changed out of fears for his safety, was forced to attend an islamic state children's campsurvivor of isis children's camp reveals how young boys are whipped and made to watch men being crucified and women stoned to death\\nin the last 10 or 15 years, most of (more than half) the terror attacks/deaths have been the result of islamic extremists.\",\n",
       " u'in the last 10 to 15 years, most (more than half) terrorist attacks have been carried out by non-muslims.',\n",
       " u\"i've already provided the proof of this.\",\n",
       " u\"if you refuse to believe, that's your problem.\",\n",
       " u'go on crossing the road every time you see a muslim on your side of the street, if that makes you feel safer.',\n",
       " u\"no, actually you have not..your 'proof' ends in 2005, which is 9 years ago..how many non-mulim terror attacks happened between 2005 and 2014, and how many muslim terror attacks happened in the same period??\",\n",
       " u'well just for the july section of that wiki link, about 26 out of 31 attacks were muslim inspired attacks.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "sent_tokenizer=nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "sents = sent_tokenizer.tokenize(raw)\n",
    "sents[101:111]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, we have sentences. Now what?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenisation is simply the process of breaking texts down into words. We already did a little bit of this in Session 1. We won't build our own tokenizer, because it's not much fun. NLTK has one we can rely on.\n",
    "\n",
    "Keep in mind that definitions of tokens are not standardised, especially for languages other than English. Serious problems arise when comparing two corpora that have been tokenised differently.\n",
    "\n",
    "> **Note:** It is also possible to use NLTK to break tokens into morphemes, syllables, or phonemes. We're not going to go down those roads, though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'no', u'greens-win', u',', u'many', u'of', u'us', u'right', u'wingers', u'want', u'to', u'stay', u'the', u'hell', u'out', u'of', u'the', u'middle', u'east', u'.'], [u'nothing', u'is', u'going', u'to', u'stop', u'that', u'sh.thole', u'of', u'the', u'world', u'from', u'tearing', u'each', u'others', u\"'\", u'throats', u'out', u'.'], [u'after', u'all', u',', u'they', u'have', u'been', u'doing', u'it', u'successfully', u'for', u'centuries', u'.'], [u'then', u'you', u'better', u'start', u'educating', u'your', u'hard', u'right', u'mates', u'about', u'greens', u\"'\", u'renewable', u'energy', u'.'], [u'sooner', u'we', u'end', u'our', u'addiction', u'to', u'arab', u'oil', u'and', u'middle', u'eastern', u'exports', u',', u'like', u'live', u'animals', u',', u'the', u'sooner', u'we', u'can', u'distance', u'ourselves', u'.'], [u'i', u'am', u'not', u'saying', u'that', u'all', u'muslims', u'are', u'like', u'this', u'.'], [u'however', u',', u'i', u'am', u'saying', u'that', u'many', u',', u'many', u'of', u'them', u'are', u'.'], [u'they', u'are', u'flying', u'under', u'the', u'radar', u'and', u'getting', u'strong', u',', u'just', u'like', u'hitler', u\"'s\", u'storm', u'troopers', u'before', u'little', u'adolph', u'rose', u'to', u'power', u'.'], [u'sensationalist', u'right', u'?'], [u'no', u'one', u'thought', u'much', u'about', u'little', u'adolph', u'and', u'his', u'brown', u'shirted', u'followers', u',', u'until', u'they', u'stormed', u'to', u'power', u',', u'whilst', u'the', u'do', u'gooders', u'stood', u'by', u'and', u'tut', u'tutted', u'.']]\n"
     ]
    }
   ],
   "source": [
    "tokenized_sents = [nltk.word_tokenize(i) for i in sents]\n",
    "print tokenized_sents[:10]\n",
    "# another view:\n",
    "# tokenized_sents[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming is the task of finding the stem of a word. So, *cats --> cat*, or *taking --> take*. It is an important task when counting words, as often the counting each inflection seperately is not particuarly helpful: forms of the verb 'to be' might seem under-represented if we could *is, are, were, was, am, be, being, been* separately. \n",
    "\n",
    "NLTK has pre-programmed stemmers, but we can build our own using some of the skills we've already learned.\n",
    "\n",
    "A stemmer is the kind of thing that would make a good function, so let's do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def stem(word):\n",
    "    for suffix in ['ing', 'ly', 'ed', 'ious', 'ies', 'ive', 'es', 's', 'ment']: # list of suffixes\n",
    "        if word.endswith(suffix):\n",
    "            return word[:-len(suffix)] # delete the suffix\n",
    "    return word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run it over some text and see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'no',\n",
       "  u'greens-win',\n",
       "  u',',\n",
       "  u'many',\n",
       "  u'of',\n",
       "  u'u',\n",
       "  u'right',\n",
       "  u'winger',\n",
       "  u'want',\n",
       "  u'to',\n",
       "  u'stay',\n",
       "  u'the',\n",
       "  u'hell',\n",
       "  u'out',\n",
       "  u'of',\n",
       "  u'the',\n",
       "  u'middle',\n",
       "  u'east',\n",
       "  u'.'],\n",
       " [u'noth',\n",
       "  u'i',\n",
       "  u'go',\n",
       "  u'to',\n",
       "  u'stop',\n",
       "  u'that',\n",
       "  u'sh.thole',\n",
       "  u'of',\n",
       "  u'the',\n",
       "  u'world',\n",
       "  u'from',\n",
       "  u'tear',\n",
       "  u'each',\n",
       "  u'other',\n",
       "  u\"'\",\n",
       "  u'throat',\n",
       "  u'out',\n",
       "  u'.'],\n",
       " [u'after',\n",
       "  u'all',\n",
       "  u',',\n",
       "  u'they',\n",
       "  u'have',\n",
       "  u'been',\n",
       "  u'do',\n",
       "  u'it',\n",
       "  u'successful',\n",
       "  u'for',\n",
       "  u'centur',\n",
       "  u'.'],\n",
       " [u'then',\n",
       "  u'you',\n",
       "  u'better',\n",
       "  u'start',\n",
       "  u'educat',\n",
       "  u'your',\n",
       "  u'hard',\n",
       "  u'right',\n",
       "  u'mat',\n",
       "  u'about',\n",
       "  u'green',\n",
       "  u\"'\",\n",
       "  u'renewable',\n",
       "  u'energy',\n",
       "  u'.'],\n",
       " [u'sooner',\n",
       "  u'we',\n",
       "  u'end',\n",
       "  u'our',\n",
       "  u'addiction',\n",
       "  u'to',\n",
       "  u'arab',\n",
       "  u'oil',\n",
       "  u'and',\n",
       "  u'middle',\n",
       "  u'eastern',\n",
       "  u'export',\n",
       "  u',',\n",
       "  u'like',\n",
       "  u'l',\n",
       "  u'animal',\n",
       "  u',',\n",
       "  u'the',\n",
       "  u'sooner',\n",
       "  u'we',\n",
       "  u'can',\n",
       "  u'distance',\n",
       "  u'ourselv',\n",
       "  u'.'],\n",
       " [u'i',\n",
       "  u'am',\n",
       "  u'not',\n",
       "  u'say',\n",
       "  u'that',\n",
       "  u'all',\n",
       "  u'muslim',\n",
       "  u'are',\n",
       "  u'like',\n",
       "  u'thi',\n",
       "  u'.'],\n",
       " [u'however',\n",
       "  u',',\n",
       "  u'i',\n",
       "  u'am',\n",
       "  u'say',\n",
       "  u'that',\n",
       "  u'many',\n",
       "  u',',\n",
       "  u'many',\n",
       "  u'of',\n",
       "  u'them',\n",
       "  u'are',\n",
       "  u'.'],\n",
       " [u'they',\n",
       "  u'are',\n",
       "  u'fly',\n",
       "  u'under',\n",
       "  u'the',\n",
       "  u'radar',\n",
       "  u'and',\n",
       "  u'gett',\n",
       "  u'strong',\n",
       "  u',',\n",
       "  u'just',\n",
       "  u'like',\n",
       "  u'hitler',\n",
       "  u\"'\",\n",
       "  u'storm',\n",
       "  u'trooper',\n",
       "  u'before',\n",
       "  u'little',\n",
       "  u'adolph',\n",
       "  u'rose',\n",
       "  u'to',\n",
       "  u'power',\n",
       "  u'.'],\n",
       " [u'sensationalist', u'right', u'?'],\n",
       " [u'no',\n",
       "  u'one',\n",
       "  u'thought',\n",
       "  u'much',\n",
       "  u'about',\n",
       "  u'little',\n",
       "  u'adolph',\n",
       "  u'and',\n",
       "  u'hi',\n",
       "  u'brown',\n",
       "  u'shirt',\n",
       "  u'follower',\n",
       "  u',',\n",
       "  u'until',\n",
       "  u'they',\n",
       "  u'storm',\n",
       "  u'to',\n",
       "  u'power',\n",
       "  u',',\n",
       "  u'whilst',\n",
       "  u'the',\n",
       "  u'do',\n",
       "  u'gooder',\n",
       "  u'stood',\n",
       "  u'by',\n",
       "  u'and',\n",
       "  u'tut',\n",
       "  u'tutt',\n",
       "  u'.']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# empty list for our output\n",
    "stemmed_sents = []\n",
    "for sent in tokenized_sents:\n",
    "    # empty list for stemmed sentence:\n",
    "    stemmed = []\n",
    "    for word in sent:\n",
    "        # append the stem of every word\n",
    "        stemmed.append(stem(word))\n",
    "    # append the stemmed sentence to the list of sentences\n",
    "    stemmed_sents.append(stemmed)\n",
    "# pretty print the output\n",
    "stemmed_sents[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the output, we can see that the stemmer works: *wingers* becomes *winger*, and *tearing* becomes *tear*. But, sometimes it does things we don't want: *Nothing* becomes *noth*, and *mate* becomes *mat*. Even so, for the learns, let's rewrite our function with a regex:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this approach has obvious limitations. So, let's rely on a purpose-built stemmer. These rely in part on dictionaries. Note the subtle differences between the two possible stemmers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens = [] \n",
    "for sent in tokenized_sents:\n",
    "    for word in sent:\n",
    "        tokens.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'no', u'greens-win', u',', u'many', u'of', u'us', u'right', u'wing', u'want', u'to', u'stay', u'the', u'hel', u'out', u'of', u'the', u'middl', u'east', u'.', u'noth', u'is', u'going', u'to', u'stop', u'that', u'sh.thole', u'of', u'the', u'world', u'from', u'tear', u'each', u'oth', u\"'\", u'throats', u'out', u'.', u'aft', u'al', u',', u'they', u'hav', u'been', u'doing', u'it', u'success', u'for', u'century', u'.', u'then', u'you', u'bet', u'start', u'educ', u'yo', u'hard', u'right', u'mat', u'about', u'green', u\"'\", u'renew', u'energy', u'.', u'soon', u'we', u'end', u'our', u'addict', u'to', u'arab', u'oil', u'and', u'middl', u'eastern', u'export', u',', u'lik', u'liv', u'anim', u',', u'the', u'soon', u'we', u'can', u'dist', u'ourselv', u'.', u'i', u'am', u'not', u'say', u'that', u'al', u'muslim', u'ar', u'lik', u'thi', u'.', u'howev']\n"
     ]
    }
   ],
   "source": [
    "# define stemmers\n",
    "lancaster = nltk.LancasterStemmer()\n",
    "porter = nltk.PorterStemmer()\n",
    "# stem each word in tokens\n",
    "stems = [lancaster.stem(t) for t in tokens]  # replace lancaster with porter here\n",
    "print stems[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that both stemmers handle some things rather poorly. The main reason for this is that they are not aware of the *word class* of any particular word: *nothing* is a noun, and nouns ending in *ing* should not have *ing* removed by the stemmer (swing, bling, ring...). Later in the course, we'll start annotating corpora with grammatical information. This improves the accuracy of stemmers a lot.\n",
    "\n",
    "> Note: stemming is not *always* the best thing to do: though *thing* is the stem of *things*, things has a unique meaning, as in *things will improve*. If we are interested in vague language, we may not want to collapse things --> thing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keywording: 'the aboutness of a text'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keywording is the process of generating a list of words that are unusually frequent in the corpus of interest. To do it, you need a *reference corpus*, or at least a *reference wordlist* to which your *target corpus* can be compared. Often, *reference corpora* take the form of very large collections of language drawn from a variety of spoken and written sources.\n",
    "\n",
    "Keywording is what generates word-clouds beside online news stories, blog posts, and the like. In combination with speech-to-text, it's used in Oxford University's [Spindle Project](http://openspires.oucs.ox.ac.uk/spindle/) to automatically archive recorded lectures with useful tags.\n",
    "\n",
    "We'll use corpkit, which relies on Spindle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#! pip install corpkit\n",
    "import corpkit\n",
    "from corpkit import keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 'isis', 716.823485041825]\n",
      "[1, 'terrorist', 498.3939674455444]\n",
      "[2, 'muslims', 424.3632631632848]\n",
      "[3, 'muslim', 423.25935057638816]\n",
      "[4, 'three', 306.8193032226369]\n",
      "[5, 'genocide', 304.39492316647016]\n",
      "[6, 'isil', 245.2290869879928]\n",
      "[7, 'iraq', 242.37140350399955]\n",
      "[8, 'time', 225.6694491556164]\n",
      "[9, 'attacks', 214.58846725527616]\n",
      "[10, 'sort', 172.9831520838031]\n",
      "[11, 'moslems', 170.87994026322178]\n",
      "[12, 'australian', 169.24670158499322]\n",
      "[13, 'work', 148.97528640889706]\n",
      "[14, 'bit', 148.83300159093574]\n",
      "[15, 'islamic', 147.7422380777213]\n",
      "[16, 'lot', 146.8443398846189]\n",
      "[17, 'good', 146.54802603896655]\n",
      "[18, 'people', 143.73019189795275]\n",
      "[19, 'things', 133.9788295328285]\n"
     ]
    }
   ],
   "source": [
    "# this tool works with raw text, not tokens!\n",
    "keys, ngrams = keywords(raw.encode(\"UTF-8\"))\n",
    "for key in keys[:20]:\n",
    "    print key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Success! We have keywords.\n",
    "\n",
    "> Keep in mind, the BNC reference corpus was created before ISIS and ISIL existed. *Moslem/moslems* is a dispreferred spelling of Muslim, used more frequently in anti-Islamic discourse. Also, it's unlikely that a transcriber of the spoken BNC would choose the Moslem spelling. *Having an inappropriate reference corpus is a common methodological problem in discourse analytic work*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can fiddle with the stemmer and BNC frequency to get different keyword lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'newstemmer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-11e65cbf2a9c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# try raising the threshold if there are still bad spellings!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mstemmed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnewstemmer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Lancaster'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'newstemmer' is not defined"
     ]
    }
   ],
   "source": [
    "# try raising the threshold if there are still bad spellings!\n",
    "stemmed = newstemmer(raw, 'Lancaster', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "keys = keywords_and_ngrams(stemmed)\n",
    "keys[0] # only keywords\n",
    "keys[1] # only n-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *You shall know a word by the company it keeps.* - J.R. Firth, 1957\n",
    "\n",
    "Collocation is a very common area of interest in corpus linguistics. Words pattern together in both expected and unexpected ways. In some contexts, *drug* and *medication* are synonymous, but it would be very rare to hear about *illicit* or *street medication*. Similarly, doctors are unlikely to prescribe the *correct* or *appropriate drug*.\n",
    "\n",
    "This kind of information may be useful to lexicographers, discourse analysts, or advanced language learners.\n",
    "\n",
    "In NLTK, collocation works from ordered lists of tokens. Let's put out tokenised sents into a single, huge list of tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'no', u'greens-win', u',', u'many', u'of', u'us', u'right', u'wingers', u'want', u'to', u'stay', u'the', u'hell', u'out', u'of', u'the', u'middle', u'east', u'.', u'nothing']\n"
     ]
    }
   ],
   "source": [
    "allwords = []\n",
    "# for each sentence,\n",
    "for sent in tokenized_sents:\n",
    "    # for each word,\n",
    "    for word in sent:\n",
    "        # make a list of all words\n",
    "        allwords.append(word)\n",
    "print allwords[:20]\n",
    "# small challenge: can you think of any other ways to do this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's feed these to an NLTK function for measuring collocations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u',', u'and'),\n",
       " (u',', u'but'),\n",
       " (u',', u'i'),\n",
       " (u',', u'the'),\n",
       " (u'.', u'and'),\n",
       " (u'.', u'i'),\n",
       " (u'.', u'if'),\n",
       " (u'.', u'it'),\n",
       " (u'.', u'the'),\n",
       " (u'.', u'we'),\n",
       " (u'.', u'you'),\n",
       " (u'?', u'?'),\n",
       " (u'and', u'the'),\n",
       " (u'do', u\"n't\"),\n",
       " (u'for', u'the'),\n",
       " (u'have', u'been'),\n",
       " (u'in', u'the'),\n",
       " (u'is', u'a'),\n",
       " (u'it', u\"'s\"),\n",
       " (u'it', u'is'),\n",
       " (u'middle', u'east'),\n",
       " (u'of', u'the'),\n",
       " (u'on', u'the'),\n",
       " (u'that', u'the'),\n",
       " (u'the', u'middle'),\n",
       " (u'the', u'us'),\n",
       " (u'the', u'world'),\n",
       " (u'they', u'are'),\n",
       " (u'to', u'be'),\n",
       " (u'to', u'the')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get all the functions needed for collocation work\n",
    "from nltk.collocations import *\n",
    "# define statistical tests for bigrams\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "# go and find bigrams\n",
    "finder = BigramCollocationFinder.from_words(allwords)\n",
    "# measure which bigrams are important and print the top 30\n",
    "sorted(finder.nbest(bigram_measures.raw_freq, 30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, that tells us a little: we can see that terrorists, Muslims and the Middle East are commonly collocating in the text. At present, we are only looking for immediately adjacent words. So, let's expand out search to a window of *five words either side*\n",
    "\n",
    "''window size'' specifies the distance at which \n",
    "two tokens can still be considered collocates\n",
    "finder = BigramCollocationFinder.from_words(allwords, window_size=5)\n",
    "sorted(finder.nbest(bigram_measures.raw_freq, 30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the appearance of very common words! Let's use NLTK's stopwords list to remove entries containing these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'...', u'..'),\n",
       " (u'...', u'...'),\n",
       " (u'2001', u'carried'),\n",
       " (u'``', u\"''\"),\n",
       " (u'``', u'...'),\n",
       " (u'around', u'world'),\n",
       " (u'attacks', u'around'),\n",
       " (u'attacks', u'since'),\n",
       " (u'attacks', u'world'),\n",
       " (u'ca', u\"n't\"),\n",
       " (u'carried', u'non-muslims'),\n",
       " (u'etc', u'etc'),\n",
       " (u'iraq', u'syria'),\n",
       " (u'let', u\"'s\"),\n",
       " (u'majority', u'around'),\n",
       " (u'majority', u'attacks'),\n",
       " (u'majority', u'terrorist'),\n",
       " (u'middle', u'east'),\n",
       " (u\"n't\", u'know'),\n",
       " (u\"n't\", u'think'),\n",
       " (u'shale', u'oil'),\n",
       " (u'since', u'2001'),\n",
       " (u'spirit', u'anzac'),\n",
       " (u'syria', u'iraq'),\n",
       " (u'terrorist', u'around'),\n",
       " (u'terrorist', u'attacks'),\n",
       " (u'terrorist', u'groups'),\n",
       " (u'terrorist', u'world'),\n",
       " (u'world', u'2001'),\n",
       " (u'world', u'since')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finder = BigramCollocationFinder.from_words(allwords, window_size=5)\n",
    "# get a list of stopwords from nltk\n",
    "ignored_words = nltk.corpus.stopwords.words('english')\n",
    "# make sure no part of the bigram is in stopwords\n",
    "finder.apply_word_filter(lambda w: len(w) < 2 or w.lower() in ignored_words)\n",
    "finder.apply_freq_filter(2)\n",
    "#print the sorted collocations\n",
    "sorted(finder.nbest(bigram_measures.raw_freq, 30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There! Now we have some interesting collocates. Finally, let's remove punctuation-only entries, or entries that are *n't*, as this is caused by different tokenisers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'2001', u'carried'),\n",
       " (u'air', u'strikes'),\n",
       " (u'around', u'2001'),\n",
       " (u'around', u'since'),\n",
       " (u'around', u'world'),\n",
       " (u'attacks', u'around'),\n",
       " (u'attacks', u'carried'),\n",
       " (u'attacks', u'since'),\n",
       " (u'attacks', u'world'),\n",
       " (u'boots', u'ground'),\n",
       " (u'carried', u'non-muslims'),\n",
       " (u'etc', u'etc'),\n",
       " (u'foreign', u'policy'),\n",
       " (u'iraq', u'syria'),\n",
       " (u'majority', u'around'),\n",
       " (u'majority', u'attacks'),\n",
       " (u'majority', u'terrorist'),\n",
       " (u'middle', u'east'),\n",
       " (u'muslim', u'groups'),\n",
       " (u'shale', u'oil'),\n",
       " (u'since', u'2001'),\n",
       " (u'since', u'carried'),\n",
       " (u'spirit', u'anzac'),\n",
       " (u'syria', u'iraq'),\n",
       " (u'terrorist', u'around'),\n",
       " (u'terrorist', u'attacks'),\n",
       " (u'terrorist', u'groups'),\n",
       " (u'terrorist', u'world'),\n",
       " (u'world', u'2001'),\n",
       " (u'world', u'since')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "finder = BigramCollocationFinder.from_words(allwords, window_size=5)\n",
    "ignored_words = nltk.corpus.stopwords.words('english')\n",
    "# anything containing letter or number\n",
    "regex = r'[A-Za-z0-9]'\n",
    "# the n't token\n",
    "nonot = r'n\\'t'\n",
    "# lots of conditions!\n",
    "finder.apply_word_filter(lambda w: len(w) < 2 or w.lower() in ignored_words or not re.match(regex, w) or re.match(nonot, w))\n",
    "finder.apply_freq_filter(2)\n",
    "sorted(finder.nbest(bigram_measures.raw_freq, 30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get a lot more info on collocation at the [NLTK homepage](http://www.nltk.org/howto/collocations.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering/n-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering is the task of finding words that are commonly **immediately** adjacent (as opposed to collocates, which may just be nearby). This is also often called n-grams: bigrams are two tokens that appear together, trigrams are three, etc.\n",
    "\n",
    "Clusters/n-grams have a spooky ability to tell us what a text is about.\n",
    "\n",
    "We can use *Spindle*/corpkit for bigram searching as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 'middle east', 34]\n",
      "[1, 'terrorist attacks', 25]\n",
      "[2, 'shale oil', 10]\n",
      "[3, 'muslim terrorist', 6]\n",
      "[4, 'wholesale genocide', 5]\n",
      "[5, 'terror attacks', 5]\n",
      "[6, 'islamic terrorism', 4]\n",
      "[7, 'long time', 3]\n",
      "[8, 'anzac day', 3]\n"
     ]
    }
   ],
   "source": [
    "# an argument here to stop keywords from being produced.\n",
    "keys, ngrams = keywords(raw.encode(\"UTF-8\"))\n",
    "for ngram in ngrams[:50]:\n",
    "    print ngram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's also a method for n-gram production in NLTK. We can use this to understand how n-gramming works.\n",
    "\n",
    "Below, we get lists of any ten adjacent tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('give', 'a', 'man', 'a', 'fish', 'and', 'you', 'feed', 'him', 'for')\n",
      "('a', 'man', 'a', 'fish', 'and', 'you', 'feed', 'him', 'for', 'a')\n",
      "('man', 'a', 'fish', 'and', 'you', 'feed', 'him', 'for', 'a', 'day;')\n",
      "('a', 'fish', 'and', 'you', 'feed', 'him', 'for', 'a', 'day;', 'teach')\n",
      "('fish', 'and', 'you', 'feed', 'him', 'for', 'a', 'day;', 'teach', 'a')\n",
      "('and', 'you', 'feed', 'him', 'for', 'a', 'day;', 'teach', 'a', 'man')\n",
      "('you', 'feed', 'him', 'for', 'a', 'day;', 'teach', 'a', 'man', 'to')\n",
      "('feed', 'him', 'for', 'a', 'day;', 'teach', 'a', 'man', 'to', 'fish')\n",
      "('him', 'for', 'a', 'day;', 'teach', 'a', 'man', 'to', 'fish', 'and')\n",
      "('for', 'a', 'day;', 'teach', 'a', 'man', 'to', 'fish', 'and', 'you')\n",
      "('a', 'day;', 'teach', 'a', 'man', 'to', 'fish', 'and', 'you', 'feed')\n",
      "('day;', 'teach', 'a', 'man', 'to', 'fish', 'and', 'you', 'feed', 'him')\n",
      "('teach', 'a', 'man', 'to', 'fish', 'and', 'you', 'feed', 'him', 'for')\n",
      "('a', 'man', 'to', 'fish', 'and', 'you', 'feed', 'him', 'for', 'a')\n",
      "('man', 'to', 'fish', 'and', 'you', 'feed', 'him', 'for', 'a', 'lifetime')\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "# define a sentence\n",
    "sentence = 'give a man a fish and you feed him for a day; teach a man to fish and you feed him for a lifetime'  \n",
    "# length of ngram\n",
    "n = 10\n",
    "# use builtin tokeniser (but we could use a different one)\n",
    "tengrams = ngrams(sentence.split(), n)\n",
    "for gram in tengrams:\n",
    "  print gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, there are plenty of tengrams in there! What we're interested in, however, is duplicated n-grams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# arguments: a text, ngram size, and minimum occurrences\n",
    "def ngrammer(text, gramsize, threshold = 4):\n",
    "    \"\"\"Get any repeating ngram containing gramsize tokens\"\"\"\n",
    "    # we need to import this in order to find the duplicates:\n",
    "    from collections import defaultdict\n",
    "    from nltk.util import ngrams\n",
    "    # a subdefinition to get duplicate lists in a list\n",
    "    def list_duplicates(seq):\n",
    "        tally = defaultdict(list)\n",
    "        for i,item in enumerate(seq):\n",
    "            tally[item].append(i)\n",
    "            # return to us the index and the ngram itself:\n",
    "        return ((len(locs),key) for key,locs in tally.items() \n",
    "               if len(locs) > threshold)\n",
    "    # get ngrams of gramsize    \n",
    "    raw_grams = ngrams(text.split(), gramsize)\n",
    "    # use our duplication detector to find duplicates\n",
    "    dupes = list_duplicates(raw_grams)\n",
    "    # return them, sorted by most frequent\n",
    "    return sorted(dupes, reverse = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that it's defined, let's run it, looking for trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(31, (u'the', u'middle', u'east')),\n",
       " (24, (u'in', u'the', u'middle')),\n",
       " (18, (u'carried', u'out', u'by')),\n",
       " (17, (u'the', u'majority', u'of')),\n",
       " (15, (u'of', u'terrorist', u'attacks')),\n",
       " (13, (u'out', u'by', u'non-muslims.')),\n",
       " (13, (u'majority', u'of', u'terrorist')),\n",
       " (12, (u'around', u'the', u'world')),\n",
       " (10, (u'we', u'need', u'to')),\n",
       " (10, (u'the', u'middle', u'east.')),\n",
       " (10, (u'terrorist', u'attacks', u'around')),\n",
       " (10, (u'have', u'been', u'carried')),\n",
       " (10, (u'been', u'carried', u'out')),\n",
       " (10, (u'attacks', u'around', u'the')),\n",
       " (9, (u'the', u'world', u'since')),\n",
       " (8, (u'to', u'do', u'with')),\n",
       " (8, (u'the', u'rest', u'of')),\n",
       " (8, (u'out', u'of', u'the')),\n",
       " (7, (u'world', u'since', u'2001')),\n",
       " (7, (u'to', u'deal', u'with')),\n",
       " (7, (u'this', u'is', u'a')),\n",
       " (7, (u'the', u'spirit', u'of')),\n",
       " (7, (u'since', u'2001', u'have')),\n",
       " (7, (u'of', u'the', u'world')),\n",
       " (7, (u'do', u'you', u'think')),\n",
       " (7, (u'2001', u'have', u'been')),\n",
       " (6, (u'you', u'are', u'a')),\n",
       " (6, (u'there', u'is', u'no')),\n",
       " (6, (u'that', u'there', u'is')),\n",
       " (6, (u'rest', u'of', u'the')),\n",
       " (6, (u'nothing', u'to', u'do')),\n",
       " (6, (u'middle', u'east', u'is')),\n",
       " (6, (u'in', u'iraq', u'and')),\n",
       " (6, (u'how', u'do', u'you')),\n",
       " (6, (u'boots', u'on', u'the')),\n",
       " (5, (u'you', u'want', u'to')),\n",
       " (5, (u'to', u'stop', u'the')),\n",
       " (5, (u'to', u'be', u'a')),\n",
       " (5, (u'this', u'is', u'not')),\n",
       " (5, (u'they', u'are', u'not')),\n",
       " (5, (u'there', u'is', u'a')),\n",
       " (5, (u'the', u'days', u'of')),\n",
       " (5, (u'terrorist', u'attacks', u'are')),\n",
       " (5, (u'syria', u'and', u'iraq')),\n",
       " (5, (u'spirit', u'of', u'anzac')),\n",
       " (5, (u'on', u'the', u'ground')),\n",
       " (5, (u'of', u'the', u'middle')),\n",
       " (5, (u'need', u'to', u'be')),\n",
       " (5, (u'most', u'terrorist', u'attacks')),\n",
       " (5, (u'middle', u'east', u'and')),\n",
       " (5, (u'is', u'not', u'a')),\n",
       " (5, (u'in', u'syria', u'and')),\n",
       " (5, (u'i', u\"don't\", u'think')),\n",
       " (5, (u'as', u'well', u'as')),\n",
       " (5, (u'are', u'carried', u'out'))]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngrammer(raw, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Too many results? Let's set a higher threshold than the default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(31, (u'the', u'middle', u'east')),\n",
       " (24, (u'in', u'the', u'middle')),\n",
       " (18, (u'carried', u'out', u'by')),\n",
       " (17, (u'the', u'majority', u'of')),\n",
       " (15, (u'of', u'terrorist', u'attacks')),\n",
       " (13, (u'out', u'by', u'non-muslims.')),\n",
       " (13, (u'majority', u'of', u'terrorist')),\n",
       " (12, (u'around', u'the', u'world'))]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngrammer(raw, 3, threshold = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concordancing with regular expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've already done a bit of concordancing. In discourse-analytic research, concordancing is often used to perform thematic categorisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 25 of 60 matches:\n",
      "urselves . i am not saying that all muslims are like this . however , i am sayi\n",
      " a result of long term migration of muslims into our country we now have a burg\n",
      " shut the gate as we now have young muslims , embracing isis ideals and as far \n",
      " of over-sensationalized crap . the muslims are actually less trouble than are \n",
      "aying the statistics will show that muslims are more likely to be criminal than\n",
      "ty . what about crimes committed by muslims whose grandparents emigrated here ,\n",
      ", would be collateral damage as the muslims promote their promise to 'have the \n",
      "anda ) greens -win , i am comparing muslims to storm troopers . do you have any\n",
      "ers . do you have any idea what the muslims are up to in iraq ? isis are commit\n",
      "uld be 'aussie ' , not muslim . all muslims ? you are on a hate speech , hiding\n",
      "iding behind a keyboard , about all muslims and that makes you a fanatical terr\n",
      "nt history ) . the sunni and shiite muslims have been killing each other for 12\n",
      "ehalf of islam , are perpetrated by muslims . well it 's your link , not mine .\n",
      "lims . that 's not to say that some muslims do n't indulge in terrorism : they \n",
      "lam . it was only 20 years ago that muslims of a certain region were asking aus\n",
      " public would have laughed it off . muslims have a tendency to take their relig\n",
      " the same level of criminality that muslims do , then rape , murder , and a hea\n",
      " for the bodycount , it appears the muslims are winning that one . how many dea\n",
      "he azzizzis had lived in peace with muslims in the middle east for thousands of\n",
      " he stated that through that period muslims had not try to convert them by forc\n",
      "he mainstream of 500,000 australian muslims do not support this ideology . he a\n",
      "heir soil , they will turn on other muslims ... their point is to kill and noth\n",
      "d through . i dont want to hate all muslims ... .but its not easy when they see\n",
      "in the world . and , just to note , muslims who do n't condemn muslim terrorist\n",
      "erned about their heads : the crazy muslims would n't think twice about choppin\n"
     ]
    }
   ],
   "source": [
    "text = nltk.Text(tokens)  # formats our tokens for concordancing\n",
    "text.concordance(\"muslims\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could even our stemmed corpus here:\n",
    "text = nltk.Text(stemmed)\n",
    "text.concordance(\"muslims\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You get no matches in the latter case, because all instances of *muslims* were stemmed to *muslim*.\n",
    "\n",
    "A problem with the NLTK concordancer is that it only works with individual tokens. What if we want to find words that end with **ment*, or words beginning with *poli**?\n",
    "\n",
    "We already searched text with Regular Expressions. It's not much more work to build regex functionality into our own concordancer.\n",
    "\n",
    "From running the code below, you can see that bracketting sections of our regex causes results to split into lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'as they woke to another day\\x92s work the people of sydney [and melbourne] learned of a joint operation between their states\\x92 police forces and the ',\n",
       "  u'australia',\n",
       "  u'n federal police, against an imminent terrorist attack in their cities.'),\n",
       " (u'four men - all ',\n",
       "  u'australia',\n",
       "  u\"n citizens - were arrested this morning as federal and state police, armed with search warrants, swooped on members of the suspected terror cell this morning in the second-largest counter-terrorism operation in the nation's history\\x85. about 400 police raided homes in the northern melbourne suburbs of glenroy, meadow heights, roxburgh park, broadmeadows, westmeadows, preston and epping. they also raided homes at carlton in inner melbourne and colac in southwestern victoria. (source)\"),\n",
       " (u\"authorities believe the group is at an advanced stage of preparing to storm an australian army base, using automatic weapons, as punishment for australia's military involvement in muslim countries. it is understood the men plan to kill as many soldiers as possible before they are themselves killed. members of the group have been observed carrying out surveillance of holsworthy barracks in western sydney and other suspicious activity around defence bases in victoria. electronic surveillance on the suspects is believed to have picked up discussions about ways to obtain weapons to carry out what would be the worst terror attack on \",\n",
       "  u'australia',\n",
       "  u'n soil.'),\n",
       " (u'what a load of over-sensationalized crap. the muslims are actually less trouble than are the ',\n",
       "  u'aussie',\n",
       "  u's. and if we had never interfered in their countries they would be no trouble to us at all.'),\n",
       " (u\"so you're saying the statistics will show that muslims are more likely to be criminal than \",\n",
       "  u'aussie',\n",
       "  u\"s? and we've been interfering in the mid east for over one hundred years now, time for us to 'pack' off out of there, don't you think!\")]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define a regex for different aussie words\n",
    "aussie = r'(aussie|australia)'\n",
    "searchpattern = re.compile(r\"(.*)\" + aussie + r\"(.*)\")\n",
    "search = re.findall(searchpattern, raw)\n",
    "search[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, it's ugly, but it works. We can see five bracketted results, each containing three strings. The first and third strings are the left-context and right-context. The second of the three strings is the search term.\n",
    "\n",
    "These three sections are, with a bit of tweaking, the same as the output given by a concordancer.\n",
    "\n",
    "Let's go ahead and turn our regex seacher into a concordancer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def concordancer(text, regex):\n",
    "    \"\"\"Concordance using regular expressions\"\"\"\n",
    "    import re\n",
    "    # limit context to 30 characters max\n",
    "    searchpattern = re.compile(r\"(.{,30})(\\b\" + regex + r\"\\b)(.{,30})\")\n",
    "    # find all instances of our regex\n",
    "    search = re.findall(searchpattern, raw)\n",
    "    for result in search:\n",
    "        #join each result with a tab, and print\n",
    "        print(\"\\t\".join(result).expandtabs(20))\n",
    "        # expand tabs helps align results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "states police forces and the           australian           federal police, against an im\n",
      "four men - all      australian           citizens - were arrested this\n",
      "tage of preparing to storm an           australian           army base, using automatic we\n",
      "apons, as punishment for                australia           's military involvement in mus\n",
      "be the worst terror attack on           australian           soil.\n",
      "lly less trouble than are the           aussies             . and if we had never interfer\n",
      "re likely to be criminal than           aussies             ? and we've been interfering i\n",
      "....muslim is a religion, and           australian           is a nationality. what about \n",
      "ou classify them as muslim or           aussie              ???\n",
      "may have been slaughtered by '          aussies             ', i don't blame christianity \n",
      "se? they would have had their           aussieness           brow-beaten into them.\n",
      "think there would be too many           australians          who would like to have a jiha\n",
      "h is a circular argument. any           australian           born muslim who commits a cri\n",
      "der your definition would be '          aussie              ', not muslim.\n",
      "so isis is not american/                australian          /musliim...\n",
      " a certain region were asking           australian           government to send troops to \n",
      "rimes against non-muslims. if           aussies              showed the same level of crim\n",
      "have the capacity to re-enter           australia           , either as a citizen or as a \n",
      ".and judging by the number of           australian           and british [madmen] that hav\n",
      "hat the mainstream of 500,000           australian           muslims do not support this i\n",
      "rs of the muslim community in           australia            were working hard to stop thi\n",
      "tony abbott has stated that             australia           's terrorist threat level will\n",
      ". by restraining them here in           australia            he is begging for a terror at\n",
      " tipping some rough times for           australia            right here.\n",
      "agenda better than to have an           australian           populace panicked and looking\n",
      "e oil deposit that would make           australia            an oil exporter as well. as \n",
      "hnics have never been good at           australian           history.\n",
      "reat man. you'd go for a good           aussie               man like him. in pakistan the\n",
      "many                australians          dont even want to fork out f\n",
      "k of making you ashamed to be           australian          ...cruel words from a cruel he\n",
      "rives on our doorstep you and           aussie               will be well and truly under \n",
      "u think that 5% of muslims in           australia            are extremists/\"potential ter\n",
      "s that are nothing to do with           australia           .\n",
      " gives the go ahead for young           australians          to head into the middle east \n",
      "and spill their blood,                  australia            will not be responsible for t\n",
      "n all have nothing to do with           australia           's foreign policy. most of the\n",
      "o that has nothing to do with           australia            either.\n",
      "so                  australia            has no foreign policy now. au\n",
      "gees have been rocking up and           australia            hasn't been involved in a war\n",
      "ir.they have shown loyalty to           aussies              and we should repay the court\n",
      "question' to abbott about how           australia            will react to the ongoing isi\n",
      " reprisal actions against the           australian           government.\n",
      "ice to stem the flow of young           australians          joining the islamist rebellio\n",
      ", will plug a security gap at           australia           s borders that is being explo\n",
      "n - which could apply to both           australian           and foreign passports - will \n",
      "who are planning an attack in           australia            because we have cancelled the\n",
      "the vast bulk of    australians          travelling to the syria and i\n",
      "ust 17 years old when he left           australia           .\n",
      "trategy to stop the exodus of           australians          to the middle east conflict z\n",
      " that jihadists who return to           australia            could launch terrorist attack\n",
      "the dragnet and the number of           australians          fighting in the middle east i\n",
      "s growing. some 150                     australians          are believed to be in syria a\n",
      "anges, asio, and possibly the           australian           federal police, would have th\n",
      "ing whether it can revoke the           australian           passports of dual nationals f\n",
      "roblematic to implement given           australia           s obligations under un treati\n",
      "e born here, with native born           australian           citizenship. where you going\n",
      "referring to how the u.s. and           australia            and allies got involved in th\n",
      " let those bastards back into           australia            i don't know. perhaps the law\n",
      "prevent them from re-entering           australia           .\n",
      " is saying but it's what most           australians          know. that's what justifies i\n",
      "the vast majority of                    australians          do not start wars. they eithe\n",
      "he invasion of iraq and nam -           australians          ended up condemning them. his\n",
      "a \"real\" man . a \"real\"                 australian           goes down that backyard, torc\n",
      "ow isis is a threat to us and           australian           domestic security, he didn't \n",
      "'m going against the grain of           australia           's entire military history (ex\n",
      "rinciple. i'm also pragmatic.           australia            has a role in global policing\n",
      "you've nagged the yanks and             australians          into this, and now you're wor\n",
      " iraq. abbott has pledged 600           australian           forces to the call to arms. m\n",
      "lar. constitutionally, as an           australian           citizen, they have a natural \n",
      "of an offence competent under           australian           law.\n",
      "                    aussie              , i am on board with what you \n",
      "le rationale to isis being an           australian           regional issue is that return\n",
      "ent keeps arguing, subject to           australian           laws for supporting terrorism\n",
      " in the wool diplomat who was           australia           's rep at the un at one time i\n",
      "inst isis is a proscribed (in           australia           ) terrorist group.\n",
      " there are 500,000 moslems in           australia           . let's just say purely as spe\n",
      "moslems out there support the           australian           christian majority or isil wh\n",
      "idence of grinning loons from           australia            holding up heads still deny t\n",
      "n the muslim community within           australia           .\n",
      " on the security council, and           australia            has a policy of supporting un\n",
      "d how do you expect all those           australians          to know which groups are list\n",
      " 10%? 20%? 99.9%? - but many           australians          sending funds or fighting in \n",
      "be (thats what the cia and us/          aust                 military are doing right now \n",
      "igion to do what you and many           australian          's and defiantly many american\n",
      ". lets see how many hot head           australians          on this forum wouldn't be cho\n",
      "sians tried the same stunt on           australia           .\n",
      "ld go into iraq again because           australians          have got themselves into a ti\n",
      "you were on a train with 1000           aussies              on it and one muslim but that\n",
      "ation from those countries to           australia            and to other wester countries\n",
      "f those arriving by boat into           australian           territory are economic migran\n",
      "g on our shores uninvited [by           australia           ] are moslems.\n",
      "all....moslems do not come to           australia            to become australians.\n",
      "moslems come to     australia            to be moslems, 1st, 2nd, and \n",
      "rking tirelessly to undermine           australian           society and australian instit\n",
      "do you know that, yadda? the           australian           government has not been proce\n",
      "and i demand that the                   australian           government provide me withasy\n",
      "ts coming from moslems in our           australian           community......because i know\n",
      "and i demand that the                   australian           government provide me with as\n",
      "walking on the streets of our           australian           community.\n",
      "tement...every good moslem in           australia            [and indeed, every good mosle\n",
      "ope. for some reason he hates           aussies              while still living here. why \n"
     ]
    }
   ],
   "source": [
    "concordancer(raw, r'aus.*?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! With six lines of code, we've officially created a function that improves on the one provided by NLTK! And think how easy it would be to add more functionality: an argument dictating the size of the window (currently 30 characters), or printing line numbers beside matches, would be pretty easy to add, as well.\n",
    "\n",
    "> Adding too much functionality is known as *feature creep*. It's often best to keep your functions simple and more varied. An old adage in programming is to *make each program do one thing well*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cells below, try concordancing a few things. Also try creating variables with concordance results, and then manipulate the lists. If you encounter problems with the way the concordancer runs, alter the function and redefine it. If you want, try implementing the window size variable!\n",
    "\n",
    "> **Tip:** If you wanted to get really creative, you could try stemming concordance or n-gram results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's the end of session three! Great work.\n",
    "\n",
    "So, some of these tasks are a little dry---seeing results as lists of words and scores isn't always a lot of fun. But ultimately, they're pretty important things to know if you want to avoid the 'black box approach', where you simply dump words into a machine and analyse what the machine spits out.\n",
    "\n",
    "Remember that almost every task in corpus linguistics/distance reading depends on how we segment our data into sentences, clauses, words, etc.\n",
    "\n",
    "Building a stemmer from scratch taught us how to use regular expressions, and their power. But, we also saw that they weren't perfect for the task. In later lessons, we'll use more advanced methods to normalise our data. \n",
    "\n",
    "*See you tomorrow!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliography"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ref:baker\"></a>\n",
    "Baker, P., Gabrielatos, C., Khosravinik, M., Krzyzanowski, M., McEnery, T., & Wodak, R. (2008). A useful methodological synergy? Combining critical discourse analysis and corpus linguistics to examine discourses of refugees and asylum seekers in the UK press. Discourse & Society, 19(3), 273-306.\n",
    "\n",
    "<a id=\"firth\"></a>\n",
    "Firth, J. (1957).  *A Synopsis of Linguistic Theory 1930-1955*. In: Studies in Linguistic Analysis, Philological Society, Oxford; reprinted in Palmer, F. (ed.) 1968 Selected Papers of J. R. Firth, Longman, Harlow.\n",
    "\n",
    "<a id=\"ref:hymes\"></a>\n",
    "Hymes, D. (1972). On communicative competence. In J. Pride & J. Holmes (Eds.), Sociolinguistics (pp. 269-293). Harmondsworth: Penguin Books. Retrieved from [http://humanidades.uprrp.edu/smjeg/reserva/Estudios%20Hispanicos/espa3246/Prof%20Sunny%20Cabrera/ESPA%203246%20-%20On%20Communicative%20Competence%20p%2053-73.pdf](http://humanidades.uprrp.edu/smjeg/reserva/Estudios%20Hispanicos/espa3246/Prof%20Sunny%20Cabrera/ESPA%203246%20-%20On%20Communicative%20Competence%20p%2053-73.pdf)\n",
    "\n",
    "<a id=\"ref:widdowson\"></a>\n",
    "Widdowson, H. G. (2000). On the limitations of linguistics applied. Applied Linguistics, 21(1), 3. Available at [http://applij.oxfordjournals.org/content/21/1/3.short](http://applij.oxfordjournals.org/content/21/1/3.short)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are a few blank cells, in case you need them for anything:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gutenberger(list_of_nums):\n",
    "    text = []\n",
    "    from urllib import urlopen\n",
    "    for num in list_of_nums:\n",
    "        num = str(num)\n",
    "        url = 'https://www.gutenberg.org/cache/epub/' + num + '/pg' + num + '.txt'\n",
    "        raw = urlopen(url).read()\n",
    "        raw = unicode(raw, 'utf-8')\n",
    "        title = [line for line in raw.splitlines() if line.startswith('Title:')]\n",
    "        if title:\n",
    "            print title[0]\n",
    "        text.append([title, raw])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "booknums = ['24510', '19073', '21592']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: The Production of Vinegar from Honey\n",
      "Title: Cocoa and Chocolate\n",
      "Title: The Art of Making Whiskey\n"
     ]
    }
   ],
   "source": [
    "books = gutenberger(booknums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "project gutenberg's the production of vinegar from honey, by gerard w bancks\r\n",
      "\r\n",
      "this ebook is for th\n",
      "the project gutenberg ebook of cocoa and chocolate, by arthur w. knapp\r\n",
      "\r\n",
      "this ebook is for the use \n",
      "project gutenberg's the art of making whiskey, by anthony boucherie\r\n",
      "\r\n",
      "this ebook is for the use of \n"
     ]
    }
   ],
   "source": [
    "for title, text in books:\n",
    "    print text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
