#!/usr/bin/ipython

#   Interrogating parsed corpora and plotting the results: additional resources
#   Author: Daniel McDonald

def surgeon(lst, criteria, remove = False, **kwargs):
    """
    Add or remove from results by regex or index.

    criteria: if string, it is a regular expression to keep/remove by.
                 if list, a list of indices to keep/remove by
    """
    import re
    import collections
    import warnings
    if isinstance(lst, tuple) is True:
        warnings.warn('No branch of results selected. Using .results ... ')
        lst = lst.results
    if remove:
        remove_string = 'remove = True'
    else:
        remove_string = 'remove = False'
    newlist = []
    if type(criteria) == str:
        regexp = re.compile(criteria)
        for item in lst:
            if remove is True:
                if type(item) == str:
                    if not re.search(regexp, item):
                        newlist.append(item)
                else:
                    if not re.search(regexp, item[0]):
                        newlist.append(item)                        
            if remove is False:
                if type(item) == str:
                    if re.search(regexp, item):
                        newlist.append(item)
                else:
                    if re.search(regexp, item[0]):
                        newlist.append(item)     
    if type(criteria) == list:
        if remove is True:
            newlist = list(lst)
            backward_indices = sorted(criteria, reverse = True)
            for index in backward_indices:
                newlist.remove(newlist[index])
        if remove is False:
            for index in criteria:
                newlist.append(lst[index])
    if 'sort_by' in kwargs:
        newlist = resorter(newlist, **kwargs)        
    totals = combiner(newlist, 'Totals', printmerge = False)
    # make into name tuple
    outputnames = collections.namedtuple('interrogation', ['query', 'results', 'totals'])
    query_options = [str(criteria), remove_string]
    #main_totals.append([u'Total', total])
    output = outputnames(query_options, newlist, totals)
    return output

def quickview(lst, n = 50, topics = False):
    """See first n words of an interrogation.

    lst: interrogator() list
    n: number of results to view
    topics: for investigation of topic subcorpora"""
    import warnings
    if isinstance(lst, tuple) is True:
        warnings.warn('No branch of results selected. Using .results ... ')
        lst = lst.results
    if type(lst[0]) == str or type(lst[0]) == unicode:
        return '0: %s: %d' % (lst[0], lst[-1][1])
    if not topics:
        out = []
        for index, item in enumerate(lst[:n]):
            # if it's interrogator result
            if type(item) == list:
                word = item[0]
                index_and_word = [str(index), word]
                as_string = ': '.join(index_and_word)
                out.append(as_string)
            else:
                out.append(item)
        return out
    if topics:
        topics = [d for d in os.listdir(path)
        if os.path.isdir(os.path.join(path,d))
        and d != 'years']
        out = []
        for corpus in topics:
            subout = []
            out.append(corpus.upper())
            sublist = lst[topics.index(corpus)]
            subout = []
            for item in sublist[:n]:
                indexnum = sublist.index(item)
                word = item[0]
                index_and_word = [str(indexnum), word]
                as_string = ': '.join(index_and_word)
                subout.append(as_string)
            out.append(subout)
    return out

def topix_search(topic_subcorpora, options, query, **kwargs):
    """Interrogates each topic subcorpus."""
    fullqueries = []
    results = [] # make empty list of results and totals
    totals = []
    for topic in topic_subcorpora: # for topic name
        result = interrogator(topic, options, query, **kwargs)
        fullqueries.append(result.query)
        try:
            results.append(result.results) # add to results
        except:
            results.append([])
        totals.append(result.totals)
    # now we should have 3x results and 3x totals, and a query
    outputnames = collections.namedtuple('interrogation', ['query', 'results', 'totals']) 
    output = outputnames(fullqueries, results, totals)
    return output

def topix_plot(title, results, fract_of = False, **kwargs):
    """Plots results from subcorpus interrogation."""
    topic_subcorpora = ['economics', 'health', 'politics']
    for index, topic in enumerate(topic_subcorpora):
        newtitle = title + ' in ' + str(topic) + ' articles' # semi-automatic titles (!)
        if not fract_of: # if counting ratios/percentages, 
            plotter(newtitle, results[index], **kwargs)
        else:
            plotter(newtitle, results[index], 
                fract_of = fract_of[index], **kwargs)



def conc(corpus, query, n = 100, random = False, window = 50, trees = False, csvmake = False): 
    """A concordancer for Tregex queries"""
    # add sorting?
    from random import randint
    import time
    from time import localtime, strftime
    import re
    from collections import defaultdict
    import pydoc
    if csvmake:
        if os.path.isfile(csvmake):
            raise ValueError("CSV error: " + csvmake + " already exists in current directory. Move it, delete it, or change the name of the new .csv file.")

    def csvmaker(csvdata, sentences, csvmake):
        """Puts conc() results into tab-separated spreadsheet form"""
        #make data utf 8
        uc_data = []
        uc_sentences = []
        for line in csvdata:
            newline = []
            for part in line:
                newpart = unicode(part, 'utf-8')
                newline.append(newpart)
            uc_data.append(newline)
        for sentence in sentences:
            newsentence = unicode(sentence, 'utf-8')
            uc_sentences.append(newsentence)
        csv = []
        # make first line
        topline = query + '\nTab separated, with window (n=' + str(len(csvdata)) + '):\n'
        midline = '\n\n' + query + '\nEntire sentences (n=' + str(len(sentences)) + '):\n'
        # title then years for top row
        csv.append(topline)
        # for each word
        for entry in uc_data:
            sentence = '\t'.join(entry)
            csv.append(sentence)
        csv.append(midline)
        for entry in uc_sentences:
            csv.append(entry)
        csv = '\n'.join(csv)
        # write the csv file?
        try:
            fo=open(csvmake,"w")
        except IOError:
            print "Error writing CSV file."
        fo.write(csv.encode("UTF-8"))
        fo.close()
        time = strftime("%H:%M:%S", localtime())
        print time + ": " + csvmake + " written to currect directory."
                
    def list_duplicates(seq):
        tally = defaultdict(list)
        for i,item in enumerate(seq):
            tally[item].append(i)
        return ((key,locs) for key,locs in tally.items() 
                        if len(locs)>1)

        ##############################################################

    time = strftime("%H:%M:%S", localtime())
    #if noprint is False:
    print "\n%s: Getting concordances for %s ... \n          Query: %s\n" % (time, corpus, query)
    output = []
    tregex_command = "sh ./tregex.sh '%s' 2>&1" % query
    testpattern = !$tregex_command
    tregex_error = re.compile(r'^Error parsing expression')
    regex_error = re.compile(r'^Exception in thread')
    if re.match(tregex_error, testpattern[0]):
        tregex_error_output = "Error parsing Tregex expression. Check for balanced parentheses and boundary delimiters."
        raise ValueError(tregex_error_output)
    if re.match(regex_error, testpattern[0]):
        info = testpattern[0].split(':')
        index_of_error = re.findall(r'index [0-9]+', info[1])
        justnum = index_of_error[0].split('dex ')
        spaces = ' ' * int(justnum[1])
        remove_start = query.split('/', 1)
        remove_end = remove_start[1].split('/', -1)
        regex_error_output = 'Error parsing regex inside Tregex query:%s '\
        '. Best guess: \n%s\n%s^' % (str(info[1]), str(remove_end[0]), spaces)
        raise ValueError(regex_error_output)    
    if trees:
        options = '-s'
    else:
        options = '-t'
    tregex_command = "sh ./tregex.sh -o -w %s '%s' %s 2>/dev/null | grep -vP '^\s*$'" % (options, query, corpus)
    # replace bracket: '-LRB- ' and ' -RRB- ' ...
    allresults = !$tregex_command
    #print allresults
    results = list(allresults)
    if csvmake: # this is not optimised at all!
        sentences = list(results)
    tregex_command = "sh ./tregex.sh -o %s '%s' %s 2>/dev/null | grep -vP '^\s*$'" % (options, query, corpus)
    alltresults = !$tregex_command
    #print alltresults
    tresults = list(alltresults)
    zipped = zip(allresults, alltresults)
    all_dupes = []
    #for dup in sorted(list_duplicates(results)):
        #index_list = dup[1][1:] # the list of indices for each duplicate, minus the first one, which we still want.
        #for i in index_list:
            #all_dupes.append(i)
    #for i in sorted(all_dupes, reverse = True):
        #print tresults[i]
        #print results[i]
    #n = len(results)
    #for i in xrange(n):
        #print tresults[i]
        #print results[i]
    totalresults = len(zipped)
    if totalresults == 0:
        raise ValueError("No matches found, sorry. I wish there was more I could tell you.") 
    maximum = len(max(tresults, key=len)) # longest result in characters
    csvdata = []
    unique_results = []
    for result in zipped: 
        tree = result[0]

        pattern = result[1]
        if not trees:
            regex = re.compile(r"(\b[^\s]{0,1}.{," + re.escape(str(window)) + r"})(\b" + 
            re.escape(pattern) + r"\b)(.{," + re.escape(str(window)) + r"}[^\s]\b)")
        else:
            regex = re.compile(r"(.{,%s})(%s)(.{,%s})" % (window, re.escape(pattern), window ))
        search = re.findall(regex, tree)
        for result in search:
            unique_results.append(result)
    unique_results = set(sorted(unique_results)) # make unique
    for unique_result in unique_results:
        lstversion = list(unique_result)
        if len(lstversion) == 3:
            # make match red!
            # lstversion[1] = "\x1b[31m%s\x1b[0m" % lstversion[1]
            if csvmake:
                csvdata.append(lstversion)
            whitespace_first = window + 2 - len(lstversion[0])
            whitespace_second = maximum - len(lstversion[1])
            lstversion[0] = ' ' * whitespace_first + lstversion[0]
            lstversion[1] = lstversion[1] + ' ' * whitespace_second
            output.append(lstversion)
    formatted_output = []
    for index, row in enumerate(output):
        formatted_output.append(" ".join(row))
        #if noprint is False:
        if not random:
            if index < n:
                print '% 4d' % index, " ".join(row)
    if csvmake:
        csvmaker(csvdata, sentences, csvmake)
    if not random:
        return formatted_output
    if random:
        outnum = len(output)
        if n > outnum:
            n = outnum
        rand_out = []
        while len(rand_out) < n:
            randomnum = randint(0,outnum - 1)
            possible = output[randomnum]
            if possible not in rand_out:
                rand_out.append(possible)
        formatted_random_output = []
        for index, row in enumerate(rand_out):
            formatted_random_output.append(" ".join(row))
            print '% 4d' % index, " ".join(row)
        return formatted_random_output

def tally(lst, indices):
    """Display total occurrences of a result"""

    # this tool doesn't do a whole lot, now that totals are found during interrogation.
    output = []
    if type(indices) == int:
        item_of_interest = lst[indices]
        word = item_of_interest[0]
        total = item_of_interest[-1][-1]
        string = str(indices) + ': ' + str(word) + ': ' + str(total) + ' total occurrences.'
        output.append(string)
    if type(indices) == list:
        for index in indices:
            item_of_interest = lst[index]
            word = item_of_interest[0]
            total = item_of_interest[-1][-1]
            string = str(index) + ': ' + str(word) + ': ' + str(total) + ' total occurrences.'
            output.append(string)
    if type(indices) == str:
        if indices == 'all':
            for item in lst:
                word = item[0]
                total = item[-1][-1]
                string = str(lst.index(item)) + ': ' + str(word) + ': ' + str(total) + ' total occurrences.'
                output.append(string)
        else: # if regex
            import re
            regex = re.compile(indices)    
            for item in lst:
                if re.search(regex, item[0]):
                    word = item[0]
                    total = item[-1][-1]
                    string = str(lst.index(item)) + ': ' + str(word) + ': ' + str(total) + ' total occurrences.'
                    output.append(string)
                #else:
                    #raise ValueError("No matches found. Sorry")
    return output

def merger(lst, criteria, newname = False, printmerge = True, sort_by = 'totals'):
    """Merges result items by their index

    lst: list to work on
    criteria: list of result indexes
    newname = if str, this becomes the new name
                        if int, the item indexed with that int becomes newname
                        if False, most common item becomes newname"""
    import re
    import collections
    import warnings
    if isinstance(lst, tuple) is True:
        warnings.warn('No branch of results selected. Using .results ... ')
        lst = lst.results
    tomerge = []
    oldlist_copy = list(lst)
    # if regex, get list of indices for mwatches
    if type(criteria) == str:
        forwards_index = []
        regex = re.compile(criteria, re.UNICODE)
        for entry in oldlist_copy:
            if re.search(regex, unicode(entry[0])):
                forwards_index.append(oldlist_copy.index(entry))
        backward_indices = sorted(forwards_index, reverse = True)
    #if indices, ensure results are sorted
    if type(criteria) == list:
        forwards_index = list(criteria)
        backward_indices = sorted(criteria, reverse = True)
    # remove old entries
    for index in backward_indices:
        oldlist_copy.remove(oldlist_copy[index])
    # add matching entries to tomerge
    for index in forwards_index:
        tomerge.append(lst[index])
    # get the new word or use the first entry
    if type(newname) == int:
        getnewname= lst[newname]
        the_newname = getnewname[0]
    elif type(newname) == str:
        the_newname = unicode(newname)
    elif type(newname) == unicode:
        the_newname = newname
    else:
        the_newname = tomerge[0][0]

    merged = combiner(tomerge, the_newname, printmerge = printmerge)
    
    # put the merged entry back in the list
    # this is a bit redundant now that there's sorting
    output = []
    if type(criteria) == list:
        forwards_index = sorted(criteria)
    first_index = forwards_index[0]
    for entry in oldlist_copy[:first_index]:
        output.append(entry)
    output.append(merged)
    for entry in oldlist_copy[first_index:]:
         output.append(entry)
    if sort_by is not False:
        output = resorter(output, sort_by = sort_by)
    # generate totals:
    totals = combiner(output, 'Totals', printmerge = False)
    # make into name tuple
    outputnames = collections.namedtuple('interrogation', ['query', 'results', 'totals'])
    query_options = [str(criteria), the_newname]
    #main_totals.append([u'Total', total])
    output = outputnames(query_options, output, totals)
    return output

def searchtree(tree, query):
    "Searches a tree with Tregex and returns matching terminals"
    ! echo "$tree" > "tmp.tree"
    tregex_command = 'sh ./tregex.sh -o -t \'' + query + '\' tmp.tree 2>/dev/null | grep -vP \'^\s*$\''
    result = !$tregex_command
    ! rm "tmp.tree"
    return result

def quicktree(tree):
    """Return a visual representation of a parse tree in IPython"""
    from nltk import Tree
    from nltk.draw.util import CanvasFrame
    from nltk.draw import TreeWidget
    from IPython.display import display
    from IPython.display import Image
    parsed = Tree.fromstring(tree)
    cf = CanvasFrame()
    tc = TreeWidget(cf.canvas(),parsed)
    cf.add_widget(tc,10,10) # (10,10) offsets
    cf.print_to_file('tree.ps')
    cf.destroy()
    ! convert tree.ps tree.png
    ! rm tree.ps
    return Image(filename='tree.png')
    ! rm tree.png

def table(data_to_table, allresults = False, maxresults = 50):
    """Outputs a table from interrogator or CSV results.

    if allresults, show all results table, rather than just plotted results"""
    import pandas
    from pandas import DataFrame
    import re
    import os
    from StringIO import StringIO
    if type(data_to_table) == str:
        f = open(data_to_table)
        raw = f.read()
        #raw = os.linesep.join([s for s in raw.splitlines() if s])
        plotted_results, all_results =  raw.split('All results:')
        if not allresults:
            lines = plotted_results.split('\n')
            print str(lines[1])
            data = '\n'.join([line for line in lines if line.strip()][2:])
        if allresults:
            lines = all_results.split('\n')
            print str(lines[1])
            lines = lines[:maxresults + 3]
            data = '\n'.join([line for line in lines if line.strip()][1:])
    elif type(data_to_table) == list:
        csv = []
        if type(data_to_table[0]) == str or type(data_to_table[0]) == unicode:
            wrapped = [list(data_to_table)]
        else:
            wrapped = list(data_to_table)
        regex = re.compile('(?i)total')
        if re.match(regex, wrapped[-1][0]):
            total_present = True
        else:
            total_present = False
        years = [str(year) for year, count in wrapped[0][1:]]
        # uncomment below to make total column
        topline = ',' + ','.join(years) # + ',total'
        csv.append(topline)
        data = []
        for entry in wrapped[:maxresults]:
            word = entry[0]
            total = sum([count for year, count in entry[1:]])
            counts = [str(count) for year, count in entry[1:]]
            # uncomment below to make total column
            dataline = str(word) + ',' + ','.join(counts) # + ',' + str(total)
            csv.append(dataline)
        # table it with pandas
        data = '\n'.join(csv)
    df = DataFrame(pandas.read_csv(StringIO(data), index_col = 0, engine='python'))
    pandas.options.display.float_format = '{:,.2f}'.format
    return df

def datareader(data):
    """Figures out what kind of thing you're parsing
    and returns a big string of text"""
    import os
    if type(data) == str:
        # if it's a file, assume csv and get the big part
        if os.path.isfile(data):
            f = open(data)
            raw = f.read()
            if data.endswith('csv'):
                bad, good = re.compile(r'Entire sentences \(n=[0-9]+\):').split(raw)
            else:
                good = raw
        # if it's a dir, conc()
        elif os.path.isdir(data):
            # why did root appear as key???
            tregex_command = "sh ./tregex.sh -o -w -t '__ !> __' %s 2>/dev/null | grep -vP '^\s*$'" % data
            trees = !$tregex_command
            #remove root!?
            trees = [tree for tree in trees if tree is not 'root']
            good = '\n'.join(trees)
            if len(trees) == 0:
                # assuming data isn't trees, so 
                # read plain text out of files ...
                list_of_texts = []
                for f in os.listdir(data):
                    raw = open(os.path.join(data, f))
                    list_of_texts.append(raw)
                good = '\n'.join(list_of_texts)
        # if a string of text, just keyword that
        else:
            good = str(data)
    # if conc results, turn into string...
    if type(data) == list:
        good = '\n'.join(data)
        #type(data) == str:
        # assume it's text
    return good

def keywords(data, dictionary = 'all.p', **kwargs):
    """Feed this a csv file generated with conc() and get its keywords"""
    #import sys
    #sys.path.insert(0, 'spindle-code-master/keywords')
    #% run corpling_tools/keywords.ipy
    from corpling_tools.spindle_keywords import ngrams, keywords_and_ngrams
    import re
    import os
    import time
    from time import localtime, strftime
    try:
        from IPython.display import display, clear_output
        have_ipython = True
    except ImportError:
        have_ipython = False
    # turn all sentences into long string
    time = strftime("%H:%M:%S", localtime())
    #if noprint is False:
    print "\n%s: Generating keywords and ngrams... \n" % time
    good = datareader(data)
    keywords, ngrams = keywords_and_ngrams(good, dictionary = dictionary, **kwargs)
    keywords_list_version = []
    for index, item in enumerate(keywords):
        aslist = [index, item[0], item[1]]
        keywords_list_version.append(aslist)
    ngrams_list_version = []
    for index, item in enumerate(ngrams):
        joined_ngram = ' '.join(item[0])
        aslist = [index, joined_ngram, item[1]]
        ngrams_list_version.append(aslist)
    clear_output()    
    return keywords_list_version, ngrams_list_version

# keywords, ngrams = keywords('test.csv', nBigrams = 4)
# print ngrams

def collocates(data, nbest = 30, window = 5):
    """Feed this a csv file generated with conc() and get its collocations"""
    import nltk
    from nltk import collocations
    from nltk.collocations import BigramCollocationFinder
    import re
    import os
    import time
    from time import localtime, strftime
    try:
        from IPython.display import display, clear_output
        have_ipython = True
    except ImportError:
        have_ipython = False
    # turn all sentences into long string
    time = strftime("%H:%M:%S", localtime())
    #if noprint is False:
    print "\n%s: Generating %d collocates ... \n" % (time, nbest)
    good = datareader(data)
    good = unicode(good.lower(), 'utf-8', errors = 'ignore')
    sent_tokenizer=nltk.data.load('tokenizers/punkt/english.pickle')
    sents = sent_tokenizer.tokenize(good)
    tokenized_sents = [nltk.word_tokenize(i) for i in sents]
    allwords = []
    # for each sentence,
    for sent in tokenized_sents:
    # for each word,
        for word in sent:
        # make a list of all words
            allwords.append(word)
    bigram_measures = nltk.collocations.BigramAssocMeasures()
    finder = BigramCollocationFinder.from_words(allwords, window_size=window)
    ignored_words = nltk.corpus.stopwords.words('english')
    # anything containing letter or number
    regex = r'[A-Za-z0-9]'
    # the n't token
    nonot = r'n\'t'
    # lots of conditions!
    finder.apply_word_filter(lambda w: len(w) < 2 or w.lower() \
        in ignored_words or not re.match(regex, w) or re.match(nonot, w))
    finder.apply_freq_filter(2)
    results = sorted(finder.nbest(bigram_measures.raw_freq, nbest))
    listversion = []
    for index, thecollocation in enumerate(results):
        aslist = [index, thecollocation[0], thecollocation[1]]
        listversion.append(aslist)
    clear_output()
    return listversion

def report_display():
    """Displays/downloads the risk report, depending on your browser settings"""
    class PDF(object):
        def __init__(self, pdf, size=(200,200)):
            self.pdf = pdf
            self.size = size
        def _repr_html_(self):
            return '<iframe src={0} width={1[0]} height={1[1]}></iframe>'.format(self.pdf, self.size)
        def _repr_latex_(self):
            return r'\includegraphics[width=1.0\textwidth]{{{0}}}'.format(self.pdf)
    return PDF('report/risk_report.pdf',size=(800,650))

# colls = collocates('test.csv')
# print colls

def parsetree(tree):
    """Parse a sentence and return a visual representation in IPython"""
    from nltk import Tree
    from nltk.draw.util import CanvasFrame
    from nltk.draw import TreeWidget
    from stat_parser import Parser
    from IPython.display import display
    from IPython.display import Image
    parser = Parser()
    parsed = parser.parse(sentence)
    cf = CanvasFrame()
    tc = TreeWidget(cf.canvas(),parsed)
    cf.add_widget(tc,10,10) # (10,10) offsets
    cf.print_to_file('tree.ps')
    cf.destroy()
    ! convert tree.ps tree.png
    ! rm tree.ps
    return Image(filename='tree.png')
    ! rm tree.png

def resorter(lst, reverse = True, sort_by = 'total'):
    """Re-sort interrogation results alphabetically or by total"""
    # consider adding this within surgeon and merger
    from operator import itemgetter # for more complex sorting
    to_reorder = list(lst)
    if sort_by == 'total':
        for item in to_reorder:
            #print item[0]
            total = sum([t[-1] for t in item[1:]])
            item.append([u'Total', total])
        to_reorder.sort(key=lambda x: x[-1], reverse = reverse)
        for item in to_reorder:
            item.pop()
    if sort_by == 'name':
        # case sensitive!
        to_reorder.sort(key=lambda x: x[0])
    return to_reorder

def multiquery(corpus, query):
    """Creates a named tuple for a list of named queries to count.

    Pass in something like:

    [[u'NPs in corpus', r'NP'], [u'VPs in corpus', r'VP']]"""

    import collections
    results = []
    for name, pattern in query:
        result = interrogator(corpus, '-C', pattern)
        result.totals[0] = name # rename count
        results.append(result.totals)
    tot = merger(results, r'.*', newname = 'Totals', printmerge = False)
    #print tot
    outputnames = collections.namedtuple('interrogation', ['query', 'results', 'totals'])
    output = outputnames(query, results, tot.totals)
    return output

def resorter(lst, sort_by = 'total', reverse = True):
    """Re-sort interrogation results alphabetically or by total"""
    from operator import itemgetter # for more complex sorting
    to_reorder = list(lst)
    if sort_by == 'total':
        #for item in to_reorder:
            #print item[0]
            # wait, are totals not already calculated?!
            #total = sum([t[-1] for t in item[1:]])
            #item.append([u'Total', total])
        to_reorder.sort(key=lambda x: x[-1], reverse = reverse)
        #for item in to_reorder:
            #item.pop()
    if sort_by == 'name':
        # case insensitive!
        to_reorder.sort(key=lambda x: x[0].lower())
    return to_reorder

def combiner(tomerge, newname, printmerge = True):
    """the main engine for merging entries, making totals"""
    toprint = []
    for entry in tomerge:
        toprint.append(unicode(entry[0]))
    string_to_print = '\n'.join(toprint)
    if printmerge is True:
        # add newname to this print
        print "Merging the following entries as '%s':\n%s" % (newname, string_to_print)
    combined = zip(*[l for l in tomerge])
    merged = [newname]
    for tup in combined[1:]: # for each tuple of combined years and counts
        getyearfrom = tup[0]
        year = getyearfrom[0]
        counts = []
        for bit in tup:
            counts.append(bit[1])
        total = sum(counts)
        goodtup = [year, total]
        merged.append(goodtup)
    return merged

def mather(oldlist, operation, newlist, multiplier = 100):
    """does simple maths on two lists of results/totals"""
    import operator
    the_oldlist = list(oldlist)
    the_newlist = list(newlist)
    if type(the_oldlist) != type(the_newlist):
        raise ValueError('Different list types: %s and %s' % (type(the_oldlist), type(the_newlist)))
    # implement when everything is definitely in unicode, maybe:
    #if type(the_oldlist[0]) != type(the_newlist[0]):
        #raise ValueError('Different list depths.' 
    if len(the_oldlist) != len(the_newlist):
        print the_oldlist
        print the_newlist
        raise ValueError('Different list lengths: %d and %d' % (len(the_oldlist), len(the_newlist)))
    ops = {"+": operator.add,
           "-": operator.sub,
           "*": operator.mul,
           "/": operator.div}
    try:
        op_func = ops[operation]
    except KeyError:
        if operation != '%':
            raise ValueError("Operator not recognised. Must be '+', '-', '*', '/' or '%'.")
    # put word into a newly declared list
    mathedlist = [the_oldlist[0]]
    for index, entry in enumerate(the_oldlist[1:]):
        x_axis = entry[0]
        oldnum = entry[1]
        newtup = the_newlist[index + 1]
        if newtup[0] == x_axis:
            newnum = the_newlist[index + 1][1]
            if operation != '%':
                result = op_func(oldnum, float(newnum))
                # one day there will probably be a divide by zero error here. sorry!
            else:
                if newnum == 0:
                    result = 0
                else:
                    result = oldnum * multiplier / float(newnum)

            mathedlist.append([x_axis, result])
        else:
            raise ValueError('Different list labels: %s and %s' % (str(x_axis), str(newtup[0])))
    return mathedlist
