#!/usr/local/bin/ipython

#   Interrogating parsed corpora and plotting the results: interrogator
#   for ResBaz NLTK stream
#   Author: Daniel McDonald

def interrogator(path, options, query, lemmatise = False, titlefilter = False, lemmatag = False, usa_english = True):
    import collections
    from collections import Counter
    import os
    import re
    from corpling_tools.progressbar import ProgressBar
    from time import localtime, strftime
    if lemmatise:
        import nltk
        from nltk.stem.wordnet import WordNetLemmatizer
        lmtzr=WordNetLemmatizer()
        # location of words for manual lemmatisation
        from data.dictionaries.word_transforms import wordlist, usa_convert

    # check if we are in ipython
    try:
        from IPython.display import display, clear_output
        have_ipython = True
    except ImportError:
        have_ipython = False

    def query_test(query):
        import re
        # define error searches 
        tregex_error = re.compile(r'^Error parsing expression')
        regex_error = re.compile(r'^Exception in thread')

        #define command and run it
        tregex_command = 'sh ./tregex.sh \'%s\' 2>&1' % (query)
        if have_ipython:
            testpattern = !$tregex_command
        else:
            testpattern = os.system(tregex_command)

        # if tregex error, give general error message
        if re.match(tregex_error, testpattern[0]):
            tregex_error_output = "Error parsing Tregex expression. Check for balanced parentheses and boundary delimiters."
            raise ValueError(tregex_error_output) 

        # if regex error, try to help
        if re.match(regex_error, testpattern[0]):
            info = testpattern[0].split(':')
            index_of_error = re.findall(r'index [0-9]+', info[1])
            justnum = index_of_error[0].split('dex ')
            spaces = ' ' * int(justnum[1])
            remove_start = query.split('/', 1)
            remove_end = remove_start[1].split('/', -1)
            regex_error_output = 'Error parsing regex inside Tregex query:%s '\
            '. Best guess: \n%s\n%s^' % (str(info[1]), str(remove_end[0]), spaces)
            raise ValueError(regex_error_output)    

    def gettag(query):
        import re
        if lemmatag is False:
            # attempt to find tag from tregex query
            # currently this will fail with a query like r'/\bthis/'
            tagfinder = re.compile(r'^[^A-Za-z]*([A-Za-z]*)') 
            tagchecker = re.compile(r'^[A-Z]{2,4}$')
            tagfinder = re.compile(r'^[^A-Za-z]*([A-Za-z]*)')
            treebank_tag = re.findall(tagfinder, query)
            if re.match(tagchecker, treebank_tag[0]):
                if treebank_tag[0].startswith('J'):
                    tag = 'a'
                elif treebank_tag[0].startswith('V'):
                    tag = 'v'
                elif treebank_tag[0].startswith('N'):
                    tag = 'n'
                elif treebank_tag[0].startswith('R'):
                    tag = 'r'
            else:
                tag = 'n' # default to noun tag---same as lemmatiser does with no tag
        if lemmatag:
            tag = lemmatag
            tagchecker = re.compile(r'^[avrn]$')
            if not re.match(tagchecker, lemmatag):
                raise ValueError("WordNet POS tag not recognised. Must be 'a', 'v', 'r' or 'n'.")
        return tag
    
    def processwords(list_of_matches):
        # encoding
        matches = [unicode(match, 'utf-8', errors = 'ignore') for match in list_of_matches]
        #lowercasing
        matches = [w.lower() for w in matches]
        if lemmatise:
            tag = gettag(query)
            matches = lemmatiser(matches, tag)
        if titlefilter:
            matches = titlefilterer(matches)
        if usa_english:
            matches = usa_english_maker(matches)
        return matches

    def lemmatiser(list_of_words, tag):
        """take a list of unicode words and a tag and return a lemmatised list."""
        tokenised_list = [nltk.word_tokenize(i) for i in list_of_words]
        output = []
        for entry in tokenised_list:
            # just get the rightmost word
            word = entry[-1]
            entry.pop()
            if word in wordlist:
                word = wordlist[word]
            word = lmtzr.lemmatize(word, tag)
            entry.append(word)
            output.append(' '.join(entry))
        return output
        # if single words: return [lmtsr.lemmatize(word, tag) for word in list_of_matches]

    def titlefilterer(list_of_matches):
        import nltk
        from data.dictionaries.titlewords import titlewords
        tokenised_list = [nltk.word_tokenize(i) for i in list_of_matches]
        output = []
        for result in tokenised_list:
            head = result[-1] # ???
            non_head = result.index(head) # ???
            title_stripped = [token for token in result[:non_head] if token.rstrip('.') not in titlewords]
            title_stripped.append(head)
            str_result = ' '.join(title_stripped)
            output.append(str_result)
        return output

    def usa_english_maker(list_of_matches):
        import nltk
        from data.dictionaries.word_transforms import usa_convert
        tokenised_list = [nltk.word_tokenize(i) for i in list_of_matches]
        output = []
        for result in tokenised_list:
            head = result[-1]
            try:
                result[-1] = usa_convert[result[-1]]
            except KeyError:
                pass
            output.append(' '.join(result))
        return output

    # welcome message based on kind of interrogation

    u_option_regex = re.compile(r'(?i)-u') # find out if u option enabled
    t_option_regex = re.compile(r'(?i)-t') # find out if t option enabled   
    o_option_regex = re.compile(r'(?i)-o') # find out if t option enabled   
    c_option_regex = re.compile(r'(?i)-c') # find out if c option enabled   
    only_count = False

    if re.match(u_option_regex, options):
        optiontext = 'Tags only.'
    if re.match(t_option_regex, options):
        optiontext = 'Terminals only.'
    if re.match(o_option_regex, options):
        optiontext = 'Tags and terminals.'
    if re.match(c_option_regex, options):
        only_count = True
        options = options.upper()
        optiontext = 'Counts only.'
    time = strftime("%H:%M:%S", localtime())
    print "%s: Begining corpus interrogation: %s\n          Query: '%s'\n          %s\n          Interrogating corpus ... \n" % (time, path, query, optiontext)
    # check that query is ok
    query_test(query)
    sorted_dirs = [d for d in os.listdir(path) if os.path.isdir(os.path.join(path,d))]
    sorted_dirs.sort(key=int)
    allwords_list = []
    results_list = []
    main_totals = [u'Totals']
    p = ProgressBar(len(sorted_dirs))
    # do interrogation with tregex
    for index, d in enumerate(sorted_dirs):
        p.animate(index)
        tregex_command = 'sh ./tregex.sh -o %s \'%s\' %s 2>/dev/null | grep -vP \'^\s*$\'' %(options, query, os.path.join(path,d))
        if have_ipython:
            result = !$tregex_command
        else:
            result = os.system(tregex_command)
        if only_count:
            tup = [int(d), int(result[0])]
            main_totals.append(tup)
            continue
        result.sort()
        main_totals.append([int(d), len(result)])
        processed_result = processwords(result)
        allwords_list.append(processed_result)
        results_list.append(processed_result)
    p.animate(len(sorted_dirs))
    if only_count:
        total = sum([i[1] for i in main_totals[1:]])
        main_totals.append([u'Total', total])
        outputnames = collections.namedtuple('interrogation', ['query', 'totals'])
        query_options = [query, options] 
        output = outputnames(query_options, main_totals)
        if have_ipython:
            clear_output()
        return output

    # flatten list
    allwords = [item for sublist in allwords_list for item in sublist]
    allwords.sort()
    unique_words = set(allwords)
    list_words = []
    for word in unique_words:
        list_words.append([word])


    # make dictionary of every subcorpus
    dicts = []
    p = ProgressBar(len(results_list))
    for index, subcorpus in enumerate(results_list):
        p.animate(index)
        subcorpus_name = sorted_dirs[index]
        dictionary = Counter(subcorpus)
        dicts.append(dictionary)
        for word in list_words:
            getval = dictionary[word[0]]
            word.append([int(subcorpus_name), getval])
    p.animate(len(results_list))
    # do totals (and keep them), then sort list by total
    for word in list_words:
        total = sum([i[1] for i in word[1:]])
        word.append([u'Total', total])
    list_words = sorted(list_words, key=lambda x: x[-1], reverse = True) # does this need to be int!?
    
    # add total to main_total
    total = sum([i[1] for i in main_totals[1:]])
    main_totals.append([u'Total', total])
    #make results into named tuple
    outputnames = collections.namedtuple('interrogation', ['query', 'results', 'totals'])
    query_options = [path, query, options] 
    output = outputnames(query_options, list_words, main_totals)
    if have_ipython:
        clear_output()
    return output
